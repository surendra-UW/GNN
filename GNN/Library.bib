
@article{zhou_graph_2020,
	title = {Graph neural networks: {A} review of methods and applications},
	volume = {1},
	issn = {2666-6510},
	url = {https://www.sciencedirect.com/science/article/pii/S2666651021000012},
	doi = {https://doi.org/10.1016/j.aiopen.2021.01.001},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
	journal = {AI Open},
	author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	year = {2020},
	keywords = {Deep learning, Graph neural network},
	pages = {57--81},
}

@book{liu_introduction_2020,
	title = {Introduction to {Graph} {Neural} {Networks}},
	url = {https://search.library.wisc.edu/catalog/9913707926802121},
	abstract = {1 online resource (XVII, 109 p.)},
	publisher = {1st ed. 2020. Cham : Springer International Publishing : Imprint: Springer, 2020.},
	author = {Liu, Zhiyuan. author},
	year = {2020},
}

@article{zhang_graph_2021,
	title = {Graph {Neural} {Networks} and {Their} {Current} {Applications} in {Bioinformatics}},
	volume = {12},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/articles/10.3389/fgene.2021.690049/full},
	doi = {10.3389/fgene.2021.690049},
	abstract = {Graph neural networks (GNNs), as a branch of deep learning in non-Euclidean space, perform particularly well in various tasks that process graph structure data. With the rapid accumulation of biological network data, GNNs have also become an important tool in bioinformatics. In this research, a systematic survey of GNNs and their advances in bioinformatics is presented from multiple perspectives. We first introduce some commonly used GNN models and their basic principles. Then, three representative tasks are proposed based on the three levels of structural information that can be learned by GNNs: node classification, link prediction, and graph generation. Meanwhile, according to the specific applications for various omics data, we categorize and discuss the related studies in three aspects: disease prediction, drug discovery, and biomedical imaging. Based on the analysis, we provide an outlook on the shortcomings of current studies and point out their developing prospect. Although GNNs have achieved excellent results in many biological tasks at present, they still face challenges in terms of low-quality data processing, methodology, and interpretability and have a long road ahead. We believe that GNNs are potentially an excellent method that solves various biological problems in bioinformatics research.},
	urldate = {2024-05-01},
	journal = {Frontiers in Genetics},
	author = {Zhang, Xiao-Meng and Liang, Li and Liu, Lin and Tang, Ming-Jing},
	month = jul,
	year = {2021},
	pages = {690049},
	file = {Full Text:C\:\\Users\\Patron\\Zotero\\storage\\48PKVRA5\\Zhang et al. - 2021 - Graph Neural Networks and Their Current Applicatio.pdf:application/pdf},
}

@misc{noauthor_introduction_nodate,
	title = {An {Introduction} to {Graph} {Neural} {Networks}: {Models} and {Applications}},
	url = {https://www.youtube.com/watch?v=zCEYiCxrL_0},
}

@misc{noauthor_intro_nodate,
	title = {Intro to graph neural networks ({ML} {Tech} {Talks})},
	url = {https://www.youtube.com/watch?v=8owQBFAHw7E},
}

@inproceedings{hamilton_inductive_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {Inductive representation learning on large graphs},
	isbn = {978-1-5108-6096-4},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	year = {2017},
	note = {event-place: Long Beach, California, USA},
	pages = {1025--1035},
}

@article{wu_comprehensive_2021,
	title = {A {Comprehensive} {Survey} on {Graph} {Neural} {Networks}},
	volume = {32},
	doi = {10.1109/TNNLS.2020.2978386},
	number = {1},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
	year = {2021},
	keywords = {Data mining, Deep learning, Feature extraction, graph autoencoder (GAE), graph convolutional networks (GCNs), graph neural networks (GNNs), graph representation learning, Kernel, Learning systems, network embedding, Neural networks, Task analysis},
	pages = {4--24},
}

@misc{noauthor_torch_geometricdatasets_nodate,
	title = {torch\_geometric.datasets},
	url = {https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html},
}

@misc{noauthor_stanford_nodate,
	title = {Stanford {CS224W}: {Machine} {Learning} with {Graphs} {\textbar} 2021 {\textbar} {Lecture} 1.1 - {Why} {Graphs}},
	url = {https://www.youtube.com/watch?v=JAB_plj2rbA&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn},
}
